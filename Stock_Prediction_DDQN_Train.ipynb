{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stock Prediction DDQN - Train.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "st6GxtpHxF7L",
        "V9dsoFtsxF7M",
        "_WDyKo3kxF7P"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzhKPMCBzwah",
        "outputId": "d8824298-c965-4c4b-b817-37666ce885cf"
      },
      "source": [
        "!pip install ptan\r\n",
        "!pip install tensorboardX"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: ptan in /usr/local/lib/python3.6/dist-packages (0.6)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from ptan) (0.17.3)\n",
            "Requirement already satisfied: torch==1.3.0 in /usr/local/lib/python3.6/dist-packages (from ptan) (1.3.0)\n",
            "Requirement already satisfied: atari-py in /usr/local/lib/python3.6/dist-packages (from ptan) (0.2.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ptan) (1.19.4)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from ptan) (4.1.2.30)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->ptan) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->ptan) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->ptan) (1.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py->ptan) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->ptan) (0.16.0)\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.19.4)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0g-40L4QtxL",
        "outputId": "54f5e8c6-a9c3-4c4e-dcf9-6c3efbf75ff2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1MMPaDSxF7A"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import gym\n",
        "import csv\n",
        "import math\n",
        "import glob\n",
        "import time\n",
        "import ptan\n",
        "import enum\n",
        "import torch\n",
        "import collections\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from gym import wrappers\n",
        "import matplotlib as mpl\n",
        "import torch.optim as optim\n",
        "from gym.utils import seeding\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from ptan.agent import TargetNet, DQNAgent\n",
        "from ptan.actions import EpsilonGreedyActionSelector\n",
        "from ptan.experience import ExperienceSourceFirstLast, ExperienceReplayBuffer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMDPUjUrzlz_",
        "outputId": "c248c595-5f68-4ae9-8d77-4ac7e9445508"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "!ls \"/content/drive/My Drive/RL_Project\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "mean_val_0.013+ENV_A+DuelingDQN.data   mean_val_0.15+ENV_B+DQN.data\n",
            "mean_val_-0.017+ENV_B+DuelingDQN.data  YNDX_150101_151231.csv\n",
            "mean_val_-0.040+ENV_B+DuelingDQN.data  YNDX_160101_161231.csv\n",
            "mean_val_0.052+ENV_A+DQN.data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AR9pqlYFd_sh"
      },
      "source": [
        "data_path = '/content/drive/My Drive/RL_Project/YNDX_160101_161231.csv'\r\n",
        "val_data_path = \"/content/drive/My Drive/RL_Project/YNDX_150101_151231.csv\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAuZHtAreSRz"
      },
      "source": [
        "retention = 10\r\n",
        "commission=0.0\r\n",
        "EPSILON = 0.02"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st6GxtpHxF7L"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq4zOrezedp6"
      },
      "source": [
        "data_fields=['open', 'high', 'low', 'close', 'volume']\r\n",
        "data_headings=['<OPEN>', '<HIGH>', '<LOW>', '<CLOSE>', '<VOL>']\r\n",
        "Records = collections.namedtuple('Records', field_names=data_fields)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGltjxBfxF7L"
      },
      "source": [
        "def create_records(data):\n",
        "  for key in data:\n",
        "    data[key]=np.array(data[key])\n",
        "  rec_collection = Records(open=data['open'], high=data['high'], low=data['low'], close=data['close'], volume=data['volume'])\n",
        "  return Records(open=rec_collection.open, \n",
        "                 high=(rec_collection.high - rec_collection.open) / rec_collection.open, \n",
        "                 low=(rec_collection.low - rec_collection.open) / rec_collection.open, \n",
        "                 close=(rec_collection.close - rec_collection.open) / rec_collection.open, \n",
        "                 volume=rec_collection.volume)\n",
        "\n",
        "def read_csv(path):\n",
        "    print(\"Dataset File\", path)\n",
        "    with open(path, 'rt', encoding='utf-8') as f:\n",
        "        reader = csv.reader(f, delimiter=',')\n",
        "        headings = next(reader)\n",
        "\n",
        "        data = dict()\n",
        "        for key in data_fields:\n",
        "          data[key]=[]\n",
        "        \n",
        "        for row in reader:\n",
        "            values = []\n",
        "            for index, key in zip([headings.index(s) for s in tuple(data_headings)], data):\n",
        "                data[key].append(float(row[index]))\n",
        "    return create_records(data)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZbYTsv4csIN"
      },
      "source": [
        "def price_files(dir_name):\r\n",
        "    result = []\r\n",
        "    for path in glob.glob(os.path.join(dir_name, \"*.csv\")):\r\n",
        "        result.append(path)\r\n",
        "    return result"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9dsoFtsxF7M"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KvwaRbxxF7M"
      },
      "source": [
        "stock_actions = {'Skip':0,'Buy':1,'Close':2}\n",
        "stock_actions_rev = {}\n",
        "for key in stock_actions:\n",
        "    stock_actions_rev[stock_actions[key]]=key"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWvayhnicvnT"
      },
      "source": [
        "class StockExchangeEnvironmentA(gym.Env):\r\n",
        "    \r\n",
        "    def __init__(self):\r\n",
        "        self._records=records\r\n",
        "        self._actions = stock_actions\r\n",
        "        self._num_actions=len(self._actions)\r\n",
        "        self._shape=(3*retention+2, )\r\n",
        "        self._action_space = gym.spaces.Discrete(n=self._num_actions)\r\n",
        "        self._observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=self._shape, dtype=np.float32)\r\n",
        "        \r\n",
        "    def reset(self):\r\n",
        "        self.open_price = 0\r\n",
        "        self._count=retention\r\n",
        "        self._records=records\r\n",
        "        self.bought = False\r\n",
        "        \r\n",
        "        results, _, __ = self.step_results(0, 1, 1)\r\n",
        "        \r\n",
        "        return results\r\n",
        "    \r\n",
        "    def step(self, action_type):\r\n",
        "        reward = 0\r\n",
        "        if isinstance(action_type, np.int64):\r\n",
        "          action_type=stock_actions_rev[action_type]\r\n",
        "        current_action = self._actions[action_type]\r\n",
        "        # if we are holding any stock \r\n",
        "        if self.bought:\r\n",
        "            # if current action is to close\r\n",
        "            if current_action == stock_actions['Close']:\r\n",
        "                reward -= commission\r\n",
        "                self.bought = False\r\n",
        "                self.open_price = 0.0           \r\n",
        "            else:\r\n",
        "                # cant do anything in this case\r\n",
        "                pass\r\n",
        "        # if we are not holding any stock\r\n",
        "        else:\r\n",
        "            # current action is to buy\r\n",
        "            if self._actions[action_type] == stock_actions['Buy']:\r\n",
        "                self.bought = True\r\n",
        "                self.open_price = self._records.open[self._count] * (self._records.close[self._count] + 1)\r\n",
        "                reward -= commission\r\n",
        "            else:\r\n",
        "                # can't do anything in this case\r\n",
        "                pass\r\n",
        "        close_initial = self._records.open[self._count] * (self._records.close[self._count] + 1)\r\n",
        "        self._count += 1\r\n",
        "        close_final = self._records.open[self._count] * (self._records.close[self._count] + 1)\r\n",
        "\r\n",
        "        results, reward, terminated = self.step_results(reward, close_initial, close_final)\r\n",
        "        \r\n",
        "        return results, reward, terminated, dict()\r\n",
        "    \r\n",
        "    def step_results(self, reward, close_initial, close_final):\r\n",
        "        if self.bought:\r\n",
        "            reward += ((close_final - close_initial)/close_initial)*100\r\n",
        "\r\n",
        "        terminated = False\r\n",
        "        terminated |= self._count >= self._records.close.shape[0]-1\r\n",
        "   \r\n",
        "        itr=0\r\n",
        "        results = np.ndarray(shape=self._shape, dtype=np.float32)\r\n",
        "        for record in range(1-retention,1):\r\n",
        "            results[itr] = self._records.high[self._count + record]\r\n",
        "            results[itr+1] = self._records.high[self._count + record]\r\n",
        "            results[itr+2] = self._records.high[self._count + record]\r\n",
        "            itr+=3\r\n",
        "\r\n",
        "        results[itr] = float(self.bought)\r\n",
        "        if not self.bought:\r\n",
        "            results[itr+1] = 0.0\r\n",
        "        else:\r\n",
        "            results[itr+1] = (self._records.open[self._count] * (self._records.close[self._count] + 1) - self.open_price) / self.open_price    \r\n",
        "        \r\n",
        "        return results, reward, terminated\r\n",
        "\r\n",
        "    def get_environment_space(self):\r\n",
        "      return self._observation_space, self._action_space "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtmyZC-8cwuj"
      },
      "source": [
        "class StockExchangeEnvironmentB(gym.Env):\r\n",
        "    \r\n",
        "    def __init__(self):\r\n",
        "        self._records=records\r\n",
        "        self._actions = stock_actions\r\n",
        "        self._num_actions=len(self._actions)\r\n",
        "        self._shape=(3*retention+2, )\r\n",
        "        self._action_space = gym.spaces.Discrete(n=self._num_actions)\r\n",
        "        self._observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=self._shape, dtype=np.float32)\r\n",
        "        \r\n",
        "    def reset(self):\r\n",
        "        self.open_price = 0\r\n",
        "        self._count=retention\r\n",
        "        self._records=records\r\n",
        "        self.bought = False\r\n",
        "        \r\n",
        "        results, _, __ = self.step_results(0, 1, 1)\r\n",
        "        \r\n",
        "        return results\r\n",
        "    \r\n",
        "    def step(self, action_type):\r\n",
        "        reward = 0\r\n",
        "        if isinstance(action_type, np.int64):\r\n",
        "          action_type=stock_actions_rev[action_type]\r\n",
        "        current_action = self._actions[action_type]\r\n",
        "        # if we are holding any stock \r\n",
        "        if self.bought:\r\n",
        "            # if current action is to close\r\n",
        "            if current_action == stock_actions['Close']:\r\n",
        "                reward -= commission\r\n",
        "                self.bought = False\r\n",
        "                self.open_price = 0.0           \r\n",
        "            else:\r\n",
        "                # cant do anything in this case\r\n",
        "                pass\r\n",
        "        # if we are not holding any stock\r\n",
        "        else:\r\n",
        "            # current action is to buy\r\n",
        "            if self._actions[action_type] == stock_actions['Buy']:\r\n",
        "                self.bought = True\r\n",
        "                self.open_price = self._records.open[self._count] * (self._records.close[self._count] + 1)\r\n",
        "                reward -= commission\r\n",
        "            else:\r\n",
        "                # can't do anything in this case\r\n",
        "                pass\r\n",
        "        close_initial = self._records.open[self._count] * (self._records.close[self._count] + 1)\r\n",
        "        self._count += 1\r\n",
        "        close_final = self._records.open[self._count] * (self._records.close[self._count] + 1)\r\n",
        "\r\n",
        "        results, reward, terminated = self.step_results(reward, close_initial, close_final)\r\n",
        "        \r\n",
        "        return results, reward, terminated, dict()\r\n",
        "    \r\n",
        "    def step_results(self, reward, close_initial, close_final):\r\n",
        "        if self.bought:\r\n",
        "            reward += (close_final - close_initial)\r\n",
        "\r\n",
        "        terminated = False\r\n",
        "        terminated |= self._count >= self._records.close.shape[0]-1\r\n",
        "   \r\n",
        "        itr=0\r\n",
        "        results = np.ndarray(shape=self._shape, dtype=np.float32)\r\n",
        "        for record in range(1-retention,1):\r\n",
        "            results[itr] = self._records.high[self._count + record]\r\n",
        "            results[itr+1] = self._records.high[self._count + record]\r\n",
        "            results[itr+2] = self._records.high[self._count + record]\r\n",
        "            itr+=3\r\n",
        "\r\n",
        "        results[itr] = float(self.bought)\r\n",
        "        if not self.bought:\r\n",
        "            results[itr+1] = 0.0\r\n",
        "        else:\r\n",
        "            results[itr+1] = (self._records.open[self._count] * (self._records.close[self._count] + 1) - self.open_price)  \r\n",
        "        \r\n",
        "        return results, reward, terminated\r\n",
        "\r\n",
        "    def get_environment_space(self):\r\n",
        "      return self._observation_space, self._action_space "
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WDyKo3kxF7P"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHNb-oQMxF7R"
      },
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, observations, actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(observations.shape[0], 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, actions.n)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "\n",
        "class DuelingDQN(nn.Module):\n",
        "    def __init__(self, observations, actions):\n",
        "        super(DuelingDQN, self).__init__()\n",
        "\n",
        "        self.value_network = nn.Sequential(\n",
        "            nn.Linear(observations.shape[0], 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "        self.surplus_network = nn.Sequential(\n",
        "            nn.Linear(observations.shape[0], 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, actions.n)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        value_network = self.value_network(x)\n",
        "        advantage_network = self.surplus_network(x)\n",
        "        return value_network + advantage_network - advantage_network.mean(dim=1, keepdim=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dfB6lCoxF7S"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kl6SkydtoI_Z"
      },
      "source": [
        "def get_state_values(states, net, splits=64):\r\n",
        "    state_values=[]\r\n",
        "    batches=np.array_split(states, splits)\r\n",
        "    for batch in batches:\r\n",
        "        observation_tensor = torch.tensor(batch).to(device)\r\n",
        "        values = net(observation_tensor)\r\n",
        "        action_code = values.max(dim=1)[0].mean().item()\r\n",
        "        state_values.append(action_code)\r\n",
        "    return np.mean(state_values)\r\n",
        "\r\n",
        "def loss(batch, net, tgt_net, gamma):\r\n",
        "    state_warehouse=[]\r\n",
        "    action_warehouse=[]\r\n",
        "    reward_warehouse=[]\r\n",
        "    terminated_warehouse=[]\r\n",
        "    terminal_state_warehouse=[]\r\n",
        "    for experience in batch:\r\n",
        "      state_warehouse.append(np.array(experience.state, copy=False))\r\n",
        "      reward_warehouse.append(experience.reward)\r\n",
        "      action_warehouse.append(experience.action)\r\n",
        "      terminated_warehouse.append(experience.last_state is None)\r\n",
        "      terminal_state_warehouse.append(np.array(experience.last_state, copy=False))\r\n",
        "      if experience.last_state is None:\r\n",
        "        terminal_state_warehouse[-1]=experience.state\r\n",
        "    \r\n",
        "    state_warehouse_values=torch.tensor(np.array(state_warehouse, copy=False)).to(device)\r\n",
        "    action_warehouse_values=torch.tensor(np.array(action_warehouse)).to(device)\r\n",
        "    reward_warehouse_values=torch.tensor(np.array(reward_warehouse, dtype=np.float32)).to(device)\r\n",
        "    terminated_warehouse_values=torch.tensor(np.array(terminated_warehouse, dtype=np.uint8)).to(device)\r\n",
        "    terminal_state_warehouse_values=torch.tensor(np.array(terminal_state_warehouse, copy=False)).to(device)\r\n",
        "\r\n",
        "    state_action_values = net(state_warehouse_values).gather(1, action_warehouse_values.unsqueeze(-1)).squeeze(-1)\r\n",
        "    next_state_actions = net(terminal_state_warehouse_values).max(1)[1]\r\n",
        "    next_state_values = tgt_net(terminal_state_warehouse_values).gather(1, next_state_actions.unsqueeze(-1)).squeeze(-1)\r\n",
        "    next_state_values[terminated_warehouse_values] = 0.0\r\n",
        "\r\n",
        "    expected_state_action_values = next_state_values.detach() * gamma + reward_warehouse_values\r\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gkr1UkxxF7W"
      },
      "source": [
        "device = torch.device(\"cpu\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-y-qkxb3hGt"
      },
      "source": [
        "REPLAY_SIZE = 100000\r\n",
        "REPLAY_INITIAL = 10000\r\n",
        "\r\n",
        "CHECKPOINT_EVERY_STEP = 99999\r\n",
        "VALIDATION_EVERY_STEP = 100000\r\n",
        "\r\n",
        "TARGET_NET_SYNC = 1000\r\n",
        "GAMMA = 0.99\r\n",
        "\r\n",
        "BATCH_SIZE = 32\r\n",
        "\r\n",
        "REWARD_STEPS = 2\r\n",
        "\r\n",
        "# LEARNING_RATE = 0.0001\r\n",
        "LEARNING_RATE = 0.01\r\n",
        "\r\n",
        "STATES_TO_EVALUATE = 1000\r\n",
        "EVAL_EVERY_STEP = 1000\r\n",
        "\r\n",
        "EPSILON_START = 1.0\r\n",
        "EPSILON_STOP = 0.1\r\n",
        "EPSILON_STEPS = 1000"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKi35QiexF7W",
        "outputId": "072c9336-2184-43f1-a9ca-a54c0e3d0037"
      },
      "source": [
        "def train_stock(environment, records, network, agent, tag=''):\n",
        "  target_network = TargetNet(network)\n",
        "  experience_source = ExperienceSourceFirstLast(environment, agent, GAMMA, steps_count=REWARD_STEPS)\n",
        "  buffer = ExperienceReplayBuffer(experience_source, REPLAY_SIZE)\n",
        "  optimizer = optim.Adam(network.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "  itr=0\n",
        "  states = None\n",
        "  optimal_mean = None\n",
        "  while itr<10000:\n",
        "      itr+=1\n",
        "      buffer.populate(1)\n",
        "      selector.epsilon = max(EPSILON_STOP, EPSILON_START - itr / EPSILON_STEPS)\n",
        "\n",
        "      if len(buffer) < REPLAY_INITIAL:\n",
        "          continue\n",
        "\n",
        "      if states is None:\n",
        "          states = buffer.sample(STATES_TO_EVALUATE)\n",
        "          states = [np.array(transition.state, copy=False) for transition in states]\n",
        "          states = np.array(states, copy=False)\n",
        "\n",
        "      if itr % EVAL_EVERY_STEP == 0:\n",
        "          mean_val = get_state_values(states, network)\n",
        "          if optimal_mean is None:\n",
        "              optimal_mean = mean_val\n",
        "              network_dictionary = network.state_dict()\n",
        "              torch.save(network_dictionary, os.path.join(\"/content/drive/My Drive/RL_Project/\", \"mean_val_\"+str(round(mean_val,3))+'+'+tag+\".data\"))\n",
        "          if optimal_mean < mean_val:\n",
        "              optimal_mean = mean_val\n",
        "              network_dictionary = network.state_dict()\n",
        "              torch.save(network_dictionary, os.path.join(\"/content/drive/My Drive/RL_Project/\", \"mean_val_\"+str(round(mean_val,3))+'+'+tag+\".data\"))\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      batch = buffer.sample(BATCH_SIZE)\n",
        "      value_loss = loss(batch, network, target_network.target_model, GAMMA ** REWARD_STEPS)\n",
        "      value_loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if itr % TARGET_NET_SYNC == 0:\n",
        "          target_network.sync()\n",
        "      if itr % CHECKPOINT_EVERY_STEP == 0:\n",
        "          torch.save(target_network.state_dict(), os.path.join(saves_path, \"checkpoint-%3d.data\" % itr // CHECKPOINT_EVERY_STEP))\n",
        "  print('Training Complete!')\n",
        "\n",
        "records = read_csv(data_path)\n",
        "environment = StockExchangeEnvironmentA()\n",
        "environment = gym.wrappers.TimeLimit(environment, max_episode_steps=1000)\n",
        "observation_space, action_space = environment.get_environment_space()\n",
        "network = DQN(observation_space, action_space).to(device)\n",
        "selector = EpsilonGreedyActionSelector(EPSILON_START)\n",
        "agent = DQNAgent(network, selector, device=device)\n",
        "train_stock(environment, records, network, agent, tag='ENV_A+DQN')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset File /content/drive/My Drive/RL_Project/YNDX_160101_161231.csv\n",
            "Training Complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlykbsosxF7Y",
        "outputId": "53db0845-da3e-4e56-8679-58dcb87566dd"
      },
      "source": [
        "environment = StockExchangeEnvironmentB()\r\n",
        "environment = gym.wrappers.TimeLimit(environment, max_episode_steps=1000)\r\n",
        "observation_space, action_space = environment.get_environment_space()\r\n",
        "network = DQN(observation_space, action_space).to(device)\r\n",
        "agent = DQNAgent(network, selector, device=device)\r\n",
        "train_stock(environment, records, network, agent, tag='ENV_B+DQN')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3x6WKfDF68al",
        "outputId": "e21c6c2d-62e4-4c16-aaa0-ef0d49055896"
      },
      "source": [
        "environment = StockExchangeEnvironmentA()\r\n",
        "environment = gym.wrappers.TimeLimit(environment, max_episode_steps=1000)\r\n",
        "observation_space, action_space = environment.get_environment_space()\r\n",
        "network = DuelingDQN(observation_space, action_space).to(device)\r\n",
        "agent = DQNAgent(network, selector, device=device)\r\n",
        "train_stock(environment, records, network, agent, tag='ENV_A+DuelingDQN')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6iFIUYz68wR",
        "outputId": "abcffb7b-a8b6-4c48-a2c5-99889849322f"
      },
      "source": [
        "environment = StockExchangeEnvironmentB()\r\n",
        "environment = gym.wrappers.TimeLimit(environment, max_episode_steps=1000)\r\n",
        "observation_space, action_space = environment.get_environment_space()\r\n",
        "network = DuelingDQN(observation_space, action_space).to(device)\r\n",
        "agent = DQNAgent(network, selector, device=device)\r\n",
        "train_stock(environment, records, network, agent, tag='ENV_B+DuelingDQN')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D31Vq1p58qG4"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}