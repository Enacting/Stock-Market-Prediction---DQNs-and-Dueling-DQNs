{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JzhKPMCBzwah",
    "outputId": "56957264-d55e-4cfb-aa3f-5dd29a592ddc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ptan in /usr/local/lib/python3.6/dist-packages (0.6)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from ptan) (1.19.4)\n",
      "Requirement already satisfied: torch==1.3.0 in /usr/local/lib/python3.6/dist-packages (from ptan) (1.3.0)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from ptan) (4.1.2.30)\n",
      "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from ptan) (0.17.3)\n",
      "Requirement already satisfied: atari-py in /usr/local/lib/python3.6/dist-packages (from ptan) (0.2.6)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym->ptan) (1.5.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->ptan) (1.4.1)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym->ptan) (1.3.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py->ptan) (1.15.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym->ptan) (0.16.0)\n",
      "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.19.4)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install ptan\n",
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j0g-40L4QtxL",
    "outputId": "f6451eb6-d909-48aa-af3b-3701dc26c308"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1MMPaDSxF7A"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gym\n",
    "import csv\n",
    "import math\n",
    "import glob\n",
    "import time\n",
    "import ptan\n",
    "import enum\n",
    "import torch\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from gym import wrappers\n",
    "import matplotlib as mpl\n",
    "import torch.optim as optim\n",
    "from gym.utils import seeding\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from ptan.agent import TargetNet, DQNAgent\n",
    "from ptan.actions import EpsilonGreedyActionSelector\n",
    "from ptan.experience import ExperienceSourceFirstLast, ExperienceReplayBuffer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nMDPUjUrzlz_",
    "outputId": "08b50ab2-eaa6-4d21-f1fb-2a08938c1714"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "mean_val_-0.017+ENV_B+DuelingDQN.data  mean_val_0.15+ENV_B+DQN.data\n",
      "mean_val_-0.040+ENV_A+DuelingDQN.data  YNDX_150101_151231.csv\n",
      "mean_val_0.052+ENV_A+DQN.data\t       YNDX_160101_161231.csv\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!ls \"/content/drive/My Drive/RL_Project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AR9pqlYFd_sh"
   },
   "outputs": [],
   "source": [
    "data_path = '/content/drive/My Drive/RL_Project/YNDX_2016.csv'\n",
    "val_data_path = \"/content/drive/My Drive/RL_Project/YNDX_2015.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAuZHtAreSRz"
   },
   "outputs": [],
   "source": [
    "retention = 10\n",
    "commission=0.0\n",
    "EPSILON = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "st6GxtpHxF7L"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zq4zOrezedp6"
   },
   "outputs": [],
   "source": [
    "data_fields=['open', 'high', 'low', 'close', 'volume']\n",
    "data_headings=['<OPEN>', '<HIGH>', '<LOW>', '<CLOSE>', '<VOL>']\n",
    "Records = collections.namedtuple('Records', field_names=data_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RGltjxBfxF7L"
   },
   "outputs": [],
   "source": [
    "def create_records(data):\n",
    "  for key in data:\n",
    "    data[key]=np.array(data[key])\n",
    "  rec_collection = Records(open=data['open'], high=data['high'], low=data['low'], close=data['close'], volume=data['volume'])\n",
    "  return Records(open=rec_collection.open, \n",
    "                 high=(rec_collection.high - rec_collection.open) / rec_collection.open, \n",
    "                 low=(rec_collection.low - rec_collection.open) / rec_collection.open, \n",
    "                 close=(rec_collection.close - rec_collection.open) / rec_collection.open, \n",
    "                 volume=rec_collection.volume)\n",
    "\n",
    "def read_csv(path):\n",
    "    print(\"Dataset File\", path)\n",
    "    with open(path, 'rt', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        headings = next(reader)\n",
    "\n",
    "        data = dict()\n",
    "        for key in data_fields:\n",
    "          data[key]=[]\n",
    "        \n",
    "        for row in reader:\n",
    "            values = []\n",
    "            for index, key in zip([headings.index(s) for s in tuple(data_headings)], data):\n",
    "                data[key].append(float(row[index]))\n",
    "    return create_records(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PZbYTsv4csIN"
   },
   "outputs": [],
   "source": [
    "def price_files(dir_name):\n",
    "    result = []\n",
    "    for path in glob.glob(os.path.join(dir_name, \"*.csv\")):\n",
    "        result.append(path)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V9dsoFtsxF7M"
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0KvwaRbxxF7M"
   },
   "outputs": [],
   "source": [
    "stock_actions = {'Skip':0,'Buy':1,'Close':2}\n",
    "stock_actions_rev = {}\n",
    "for key in stock_actions:\n",
    "    stock_actions_rev[stock_actions[key]]=key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IWvayhnicvnT"
   },
   "outputs": [],
   "source": [
    "class StockExchangeEnvironmentA(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._records=records\n",
    "        self._actions = stock_actions\n",
    "        self._num_actions=len(self._actions)\n",
    "        self._shape=(3*retention+2, )\n",
    "        self._action_space = gym.spaces.Discrete(n=self._num_actions)\n",
    "        self._observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=self._shape, dtype=np.float32)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.open_price = 0\n",
    "        self._count=retention\n",
    "        self._records=records\n",
    "        self.bought = False\n",
    "        \n",
    "        results, _, __ = self.step_results(0, 1, 1)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def step(self, action_type):\n",
    "        reward = 0\n",
    "        if isinstance(action_type, np.int64):\n",
    "          action_type=stock_actions_rev[action_type]\n",
    "        current_action = self._actions[action_type]\n",
    "        # if we are holding any stock \n",
    "        if self.bought:\n",
    "            # if current action is to close\n",
    "            if current_action == stock_actions['Close']:\n",
    "                reward -= commission\n",
    "                self.bought = False\n",
    "                self.open_price = 0.0           \n",
    "            else:\n",
    "                # cant do anything in this case\n",
    "                pass\n",
    "        # if we are not holding any stock\n",
    "        else:\n",
    "            # current action is to buy\n",
    "            if self._actions[action_type] == stock_actions['Buy']:\n",
    "                self.bought = True\n",
    "                self.open_price = self._records.open[self._count] * (self._records.close[self._count] + 1)\n",
    "                reward -= commission\n",
    "            else:\n",
    "                # can't do anything in this case\n",
    "                pass\n",
    "        close_initial = self._records.open[self._count] * (self._records.close[self._count] + 1)\n",
    "        self._count += 1\n",
    "        close_final = self._records.open[self._count] * (self._records.close[self._count] + 1)\n",
    "\n",
    "        results, reward, terminated = self.step_results(reward, close_initial, close_final)\n",
    "        \n",
    "        return results, reward, terminated, dict()\n",
    "    \n",
    "    def step_results(self, reward, close_initial, close_final):\n",
    "        if self.bought:\n",
    "            reward += ((close_final - close_initial)/close_initial)*100\n",
    "\n",
    "        terminated = False\n",
    "        terminated |= self._count >= self._records.close.shape[0]-1\n",
    "   \n",
    "        itr=0\n",
    "        results = np.ndarray(shape=self._shape, dtype=np.float32)\n",
    "        for record in range(1-retention,1):\n",
    "            results[itr] = self._records.high[self._count + record]\n",
    "            results[itr+1] = self._records.high[self._count + record]\n",
    "            results[itr+2] = self._records.high[self._count + record]\n",
    "            itr+=3\n",
    "\n",
    "        results[itr] = float(self.bought)\n",
    "        if not self.bought:\n",
    "            results[itr+1] = 0.0\n",
    "        else:\n",
    "            results[itr+1] = (self._records.open[self._count] * (self._records.close[self._count] + 1) - self.open_price) / self.open_price    \n",
    "        \n",
    "        return results, reward, terminated\n",
    "\n",
    "    def get_environment_space(self):\n",
    "      return self._observation_space, self._action_space "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtmyZC-8cwuj"
   },
   "outputs": [],
   "source": [
    "class StockExchangeEnvironmentB(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._records=records\n",
    "        self._actions = stock_actions\n",
    "        self._num_actions=len(self._actions)\n",
    "        self._shape=(3*retention+2, )\n",
    "        self._action_space = gym.spaces.Discrete(n=self._num_actions)\n",
    "        self._observation_space = gym.spaces.Box(low=-np.inf, high=np.inf, shape=self._shape, dtype=np.float32)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.open_price = 0\n",
    "        self._count=retention\n",
    "        self._records=records\n",
    "        self.bought = False\n",
    "        \n",
    "        results, _, __ = self.step_results(0, 1, 1)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def step(self, action_type):\n",
    "        reward = 0\n",
    "        if isinstance(action_type, np.int64):\n",
    "          action_type=stock_actions_rev[action_type]\n",
    "        current_action = self._actions[action_type]\n",
    "        # if we are holding any stock \n",
    "        if self.bought:\n",
    "            # if current action is to close\n",
    "            if current_action == stock_actions['Close']:\n",
    "                reward -= commission\n",
    "                self.bought = False\n",
    "                self.open_price = 0.0           \n",
    "            else:\n",
    "                # cant do anything in this case\n",
    "                pass\n",
    "        # if we are not holding any stock\n",
    "        else:\n",
    "            # current action is to buy\n",
    "            if self._actions[action_type] == stock_actions['Buy']:\n",
    "                self.bought = True\n",
    "                self.open_price = self._records.open[self._count] * (self._records.close[self._count] + 1)\n",
    "                reward -= commission\n",
    "            else:\n",
    "                # can't do anything in this case\n",
    "                pass\n",
    "        close_initial = self._records.open[self._count] * (self._records.close[self._count] + 1)\n",
    "        self._count += 1\n",
    "        close_final = self._records.open[self._count] * (self._records.close[self._count] + 1)\n",
    "\n",
    "        results, reward, terminated = self.step_results(reward, close_initial, close_final)\n",
    "        \n",
    "        return results, reward, terminated, dict()\n",
    "    \n",
    "    def step_results(self, reward, close_initial, close_final):\n",
    "        if self.bought:\n",
    "            reward += (close_final - close_initial)\n",
    "\n",
    "        terminated = False\n",
    "        terminated |= self._count >= self._records.close.shape[0]-1\n",
    "   \n",
    "        itr=0\n",
    "        results = np.ndarray(shape=self._shape, dtype=np.float32)\n",
    "        for record in range(1-retention,1):\n",
    "            results[itr] = self._records.high[self._count + record]\n",
    "            results[itr+1] = self._records.high[self._count + record]\n",
    "            results[itr+2] = self._records.high[self._count + record]\n",
    "            itr+=3\n",
    "\n",
    "        results[itr] = float(self.bought)\n",
    "        if not self.bought:\n",
    "            results[itr+1] = 0.0\n",
    "        else:\n",
    "            results[itr+1] = (self._records.open[self._count] * (self._records.close[self._count] + 1) - self.open_price)  \n",
    "        \n",
    "        return results, reward, terminated\n",
    "\n",
    "    def get_environment_space(self):\n",
    "      return self._observation_space, self._action_space "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WDyKo3kxF7P"
   },
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mHNb-oQMxF7R"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, observations, actions):\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(observations.shape[0], 20),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(20, 10),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(10, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(3, actions.n)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, observations, actions):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "\n",
    "        self.value_network = nn.Sequential(\n",
    "            nn.Linear(observations.shape[0], 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "        self.surplus_network = nn.Sequential(\n",
    "            nn.Linear(observations.shape[0], 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, actions.n)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        value_network = self.value_network(x)\n",
    "        advantage_network = self.surplus_network(x)\n",
    "        return value_network + advantage_network - advantage_network.mean(dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dfB6lCoxF7S"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kl6SkydtoI_Z"
   },
   "outputs": [],
   "source": [
    "def get_state_values(states, net, splits=64):\n",
    "    state_values=[]\n",
    "    batches=np.array_split(states, splits)\n",
    "    for batch in batches:\n",
    "        observation_tensor = torch.tensor(batch).to(device)\n",
    "        values = net(observation_tensor)\n",
    "        action_code = values.max(dim=1)[0].mean().item()\n",
    "        state_values.append(action_code)\n",
    "    return np.mean(state_values)\n",
    "\n",
    "def loss(batch, net, tgt_net, gamma):\n",
    "    state_warehouse=[]\n",
    "    action_warehouse=[]\n",
    "    reward_warehouse=[]\n",
    "    terminated_warehouse=[]\n",
    "    terminal_state_warehouse=[]\n",
    "    for experience in batch:\n",
    "      state_warehouse.append(np.array(experience.state, copy=False))\n",
    "      reward_warehouse.append(experience.reward)\n",
    "      action_warehouse.append(experience.action)\n",
    "      terminated_warehouse.append(experience.last_state is None)\n",
    "      terminal_state_warehouse.append(np.array(experience.last_state, copy=False))\n",
    "      if experience.last_state is None:\n",
    "        terminal_state_warehouse[-1]=experience.state\n",
    "    \n",
    "    state_warehouse_values=torch.tensor(np.array(state_warehouse, copy=False)).to(device)\n",
    "    action_warehouse_values=torch.tensor(np.array(action_warehouse)).to(device)\n",
    "    reward_warehouse_values=torch.tensor(np.array(reward_warehouse, dtype=np.float32)).to(device)\n",
    "    terminated_warehouse_values=torch.tensor(np.array(terminated_warehouse, dtype=np.uint8)).to(device)\n",
    "    terminal_state_warehouse_values=torch.tensor(np.array(terminal_state_warehouse, copy=False)).to(device)\n",
    "\n",
    "    state_action_values = net(state_warehouse_values).gather(1, action_warehouse_values.unsqueeze(-1)).squeeze(-1)\n",
    "    next_state_actions = net(terminal_state_warehouse_values).max(1)[1]\n",
    "    next_state_values = tgt_net(terminal_state_warehouse_values).gather(1, next_state_actions.unsqueeze(-1)).squeeze(-1)\n",
    "    next_state_values[terminated_warehouse_values] = 0.0\n",
    "\n",
    "    expected_state_action_values = next_state_values.detach() * gamma + reward_warehouse_values\n",
    "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gkr1UkxxF7W"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-y-qkxb3hGt"
   },
   "outputs": [],
   "source": [
    "REPLAY_SIZE = 100000\n",
    "REPLAY_INITIAL = 10000\n",
    "\n",
    "CHECKPOINT_EVERY_STEP = 99999\n",
    "VALIDATION_EVERY_STEP = 100000\n",
    "\n",
    "TARGET_NET_SYNC = 1000\n",
    "GAMMA = 0.99\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "REWARD_STEPS = 2\n",
    "\n",
    "# LEARNING_RATE = 0.0001\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "STATES_TO_EVALUATE = 1000\n",
    "EVAL_EVERY_STEP = 1000\n",
    "\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_STOP = 0.1\n",
    "EPSILON_STEPS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EKi35QiexF7W",
    "outputId": "d63b2e6a-ea64-4dc7-bb69-9990c2a21450"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset File /content/drive/My Drive/RL_Project/YNDX_160101_161231.csv\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "def train_stock(environment, records, network, agent, tag=''):\n",
    "  target_network = TargetNet(network)\n",
    "  experience_source = ExperienceSourceFirstLast(environment, agent, GAMMA, steps_count=REWARD_STEPS)\n",
    "  buffer = ExperienceReplayBuffer(experience_source, REPLAY_SIZE)\n",
    "  optimizer = optim.Adam(network.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "  itr=0\n",
    "  states = None\n",
    "  optimal_mean = None\n",
    "  while itr<10000:\n",
    "      itr+=1\n",
    "      buffer.populate(1)\n",
    "      selector.epsilon = max(EPSILON_STOP, EPSILON_START - itr / EPSILON_STEPS)\n",
    "\n",
    "      if len(buffer) < REPLAY_INITIAL:\n",
    "          continue\n",
    "\n",
    "      if states is None:\n",
    "          states = buffer.sample(STATES_TO_EVALUATE)\n",
    "          states = [np.array(transition.state, copy=False) for transition in states]\n",
    "          states = np.array(states, copy=False)\n",
    "\n",
    "      if itr % EVAL_EVERY_STEP == 0:\n",
    "          mean_val = get_state_values(states, network)\n",
    "          if optimal_mean is None:\n",
    "              optimal_mean = mean_val\n",
    "              network_dictionary = network.state_dict()\n",
    "              torch.save(network_dictionary, os.path.join(\"/content/drive/My Drive/RL_Project/\", \"mean_val_\"+str(round(mean_val,3))+'+'+tag+\".data\"))\n",
    "          if optimal_mean < mean_val:\n",
    "              optimal_mean = mean_val\n",
    "              network_dictionary = network.state_dict()\n",
    "              torch.save(network_dictionary, os.path.join(\"/content/drive/My Drive/RL_Project/\", \"mean_val_\"+str(round(mean_val,3))+'+'+tag+\".data\"))\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      batch = buffer.sample(BATCH_SIZE)\n",
    "      value_loss = loss(batch, network, target_network.target_model, GAMMA ** REWARD_STEPS)\n",
    "      value_loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      if itr % TARGET_NET_SYNC == 0:\n",
    "          target_network.sync()\n",
    "      if itr % CHECKPOINT_EVERY_STEP == 0:\n",
    "          torch.save(target_network.state_dict(), os.path.join(saves_path, \"checkpoint-%3d.data\" % itr // CHECKPOINT_EVERY_STEP))\n",
    "  print('Training Complete!')\n",
    "\n",
    "records = read_csv(data_path)\n",
    "environment = StockExchangeEnvironmentA()\n",
    "environment = gym.wrappers.TimeLimit(environment, max_episode_steps=1000)\n",
    "observation_space, action_space = environment.get_environment_space()\n",
    "network = DQN(observation_space, action_space).to(device)\n",
    "selector = EpsilonGreedyActionSelector(EPSILON_START)\n",
    "agent = DQNAgent(network, selector, device=device)\n",
    "train_stock(environment, records, network, agent, tag='ENV_A+DQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AlykbsosxF7Y",
    "outputId": "0756cf24-a988-4037-f9d8-425575a13df1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "environment = StockExchangeEnvironmentB()\n",
    "environment = gym.wrappers.TimeLimit(environment, max_episode_steps=1000)\n",
    "observation_space, action_space = environment.get_environment_space()\n",
    "network = DQN(observation_space, action_space).to(device)\n",
    "agent = DQNAgent(network, selector, device=device)\n",
    "train_stock(environment, records, network, agent, tag='ENV_B+DQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3x6WKfDF68al",
    "outputId": "6cd87e58-c130-4b78-8dd2-6f76ac1f20da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "environment = StockExchangeEnvironmentA()\n",
    "environment = gym.wrappers.TimeLimit(environment, max_episode_steps=1000)\n",
    "observation_space, action_space = environment.get_environment_space()\n",
    "network = DuelingDQN(observation_space, action_space).to(device)\n",
    "agent = DQNAgent(network, selector, device=device)\n",
    "train_stock(environment, records, network, agent, tag='ENV_A+DuelingDQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p6iFIUYz68wR",
    "outputId": "2783fb6d-f699-467b-80ef-f8e31fff5a5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "environment = StockExchangeEnvironmentB()\n",
    "environment = gym.wrappers.TimeLimit(environment, max_episode_steps=1000)\n",
    "observation_space, action_space = environment.get_environment_space()\n",
    "network = DuelingDQN(observation_space, action_space).to(device)\n",
    "agent = DQNAgent(network, selector, device=device)\n",
    "train_stock(environment, records, network, agent, tag='ENV_B+DuelingDQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D31Vq1p58qG4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Stock Prediction DDQN - Train.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
